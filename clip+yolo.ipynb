{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75d8a3-85e7-4e4a-8676-a4d7560cc24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Loading YOLO weights: runs/detect/train/weights/best.pt on device cuda\n",
      "[info] Loading CLIP ViT-B/32 on cuda\n",
      "[info] Resuming. Found existing results with 13949 processed images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                                  | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00003.jpg: 608x640 2 objects-b8uQs, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "[warn] drawing annotations failed for 00003: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00004.jpg: 608x640 5 objects-b8uQs, 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                        | 2/50000 [00:00<1:05:24, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00004: 'ImageDraw' object has no attribute 'textsize'\n",
      "[warn] BIN_FCSKU_DATA empty for 00013, skipping\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00014.jpg: 384x640 3 objects-b8uQs, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[warn] drawing annotations failed for 00014: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00015.jpg: 512x640 5 objects-b8uQs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                          | 5/50000 [00:00<41:37, 20.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00015: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00016.jpg: 512x640 3 objects-b8uQs, 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "[warn] drawing annotations failed for 00016: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00017.jpg: 512x640 4 objects-b8uQs, 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "[warn] drawing annotations failed for 00017: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00023.jpg: 640x640 5 objects-b8uQs, 6.7ms\n",
      "Speed: 2.3ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                          | 8/50000 [00:00<41:03, 20.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00023: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00025.jpg: 640x576 3 objects-b8uQs, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "[warn] drawing annotations failed for 00025: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00030.jpg: 640x512 4 objects-b8uQs, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "[warn] drawing annotations failed for 00030: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00031.jpg: 640x512 3 objects-b8uQs, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 11/50000 [00:00<42:34, 19.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00031: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00038.jpg: 608x640 3 objects-b8uQs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "[warn] drawing annotations failed for 00038: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00039.jpg: 640x640 6 objects-b8uQs, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 14/50000 [00:00<39:05, 21.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00039: 'ImageDraw' object has no attribute 'textsize'\n",
      "[warn] BIN_FCSKU_DATA empty for 00040, skipping\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00046.jpg: 448x640 4 objects-b8uQs, 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[warn] drawing annotations failed for 00046: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00051.jpg: 480x640 4 objects-b8uQs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[warn] drawing annotations failed for 00051: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00059.jpg: 640x512 4 objects-b8uQs, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 17/50000 [00:00<43:31, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00059: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00066.jpg: 608x640 5 objects-b8uQs, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "[warn] drawing annotations failed for 00066: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00073.jpg: 640x512 5 objects-b8uQs, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 19/50000 [00:01<47:58, 17.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00073: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00084.jpg: 576x640 2 objects-b8uQs, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "[warn] drawing annotations failed for 00084: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00091.jpg: 384x640 5 objects-b8uQs, 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 21/50000 [00:01<49:59, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00091: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00098.jpg: 384x640 5 objects-b8uQs, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[warn] drawing annotations failed for 00098: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00102.jpg: 576x640 3 objects-b8uQs, 6.7ms\n",
      "Speed: 2.1ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 23/50000 [00:01<50:32, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00102: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00105.jpg: 512x640 5 objects-b8uQs, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "[warn] drawing annotations failed for 00105: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00109.jpg: 448x640 7 objects-b8uQs, 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 25/50000 [00:01<51:38, 16.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00109: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00114.jpg: 448x640 2 objects-b8uQs, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "[warn] drawing annotations failed for 00114: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00144.jpg: 608x640 5 objects-b8uQs, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 27/50000 [00:01<49:39, 16.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00144: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00151.jpg: 480x640 6 objects-b8uQs, 9.4ms\n",
      "Speed: 1.6ms preprocess, 9.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[warn] drawing annotations failed for 00151: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00156.jpg: 480x640 5 objects-b8uQs, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 29/50000 [00:01<50:36, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00156: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00158.jpg: 576x640 6 objects-b8uQs, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "[warn] drawing annotations failed for 00158: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00162.jpg: 576x640 3 objects-b8uQs, 6.4ms\n",
      "Speed: 1.8ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 31/50000 [00:01<50:27, 16.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00162: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00168.jpg: 384x640 6 objects-b8uQs, 6.9ms\n",
      "Speed: 1.2ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[warn] drawing annotations failed for 00168: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00184.jpg: 448x640 2 objects-b8uQs, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All images:   0%|                                                                         | 33/50000 [00:01<49:34, 16.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] drawing annotations failed for 00184: 'ImageDraw' object has no attribute 'textsize'\n",
      "\n",
      "image 1/1 /dgxa_home/se22uari080/sub_bin_images/00196.jpg: 576x640 6 objects-b8uQs, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n"
     ]
    }
   ],
   "source": [
    "# pipeline_v2_run_all_full.py\n",
    "# Fully-hardened, GPU-optimized bin verification pipeline (CLIP + size-ratio + Hungarian)\n",
    "# Includes: long-title shortening for CLIP, text-embedding caching, resume support, incremental saves.\n",
    "# Usage: run in one notebook cell (same environment as before). Requires ultralytics, CLIP, torch, scipy, PIL, pandas.\n",
    "\n",
    "import os, random, json, math, time, re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torch\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_DIR = \"sub_bin_images\"           # raw images directory\n",
    "META_DIR = \"sub_metadata\"           # metadata json dir\n",
    "YOLO_WEIGHTS = \"runs/detect/train/weights/best.pt\"\n",
    "OUT_DIR = Path(\"outputs/classified\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ANNOTATED_DIR = OUT_DIR / \"images\"\n",
    "ANNOTATED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULT_CSV = OUT_DIR / \"results_testing1.csv\"\n",
    "RANDOM_SEED = 42\n",
    "BATCH_EMBED_SIZE = 32                # CLIP crop batch size (reduce if OOM)\n",
    "ALPHA = 0.6                          # visual weight\n",
    "BETA = 0.4                           # size weight\n",
    "CLIP_MODEL_NAME = \"ViT-B/32\"         # CLIP model\n",
    "FONT_FILE = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "VERBOSE = True\n",
    "SAVE_EVERY = 50                      # save results every N images processed\n",
    "SHORTEN_WORDS = 12                   # words to keep from long product titles for CLIP\n",
    "# ----------------------------------------\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = DEVICE\n",
    "\n",
    "# ---------- Try import CLIP ----------\n",
    "try:\n",
    "    import clip\n",
    "    CLIP_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    CLIP_AVAILABLE = False\n",
    "    raise RuntimeError(\"CLIP not available. Install: pip install git+https://github.com/openai/CLIP.git\") from e\n",
    "\n",
    "# ---------- Load YOLO ----------\n",
    "from ultralytics import YOLO\n",
    "print(f\"[info] Loading YOLO weights: {YOLO_WEIGHTS} on device {device}\")\n",
    "yolo = YOLO(YOLO_WEIGHTS)\n",
    "try:\n",
    "    yolo.model.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- Load CLIP ----------\n",
    "print(f\"[info] Loading CLIP {CLIP_MODEL_NAME} on {device}\")\n",
    "clip_model, clip_preprocess = clip.load(CLIP_MODEL_NAME, device=device)\n",
    "clip_model.eval()\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_get_info(info, key):\n",
    "    if not info:\n",
    "        return None\n",
    "    v = info.get(key, None)\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, dict):\n",
    "        return v.get(\"value\", None)\n",
    "    return v\n",
    "\n",
    "def load_image(path):\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def crop_box(pil_img, xyxy):\n",
    "    x1, y1, x2, y2 = map(int, xyxy)\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = max(1, x2), max(1, y2)\n",
    "    return pil_img.crop((x1, y1, x2, y2))\n",
    "\n",
    "def box_metrics(xyxy):\n",
    "    x1,y1,x2,y2 = xyxy\n",
    "    w = max(1, int(x2) - int(x1))\n",
    "    h = max(1, int(y2) - int(y1))\n",
    "    return {\"w\": w, \"h\": h, \"area\": w*h, \"max_dim\": max(w,h)}\n",
    "\n",
    "def normalize_safe(mat):\n",
    "    mat = np.array(mat, dtype=float)\n",
    "    mat = np.nan_to_num(mat, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "    mn = float(np.min(mat))\n",
    "    mx = float(np.max(mat))\n",
    "    if mx - mn < 1e-12:\n",
    "        return np.zeros_like(mat)\n",
    "    return (mat - mn) / (mx - mn + 1e-9)\n",
    "\n",
    "def hungarian_from_cost(cost):\n",
    "    if cost is None:\n",
    "        return []\n",
    "    cost = np.array(cost, dtype=float)\n",
    "    if cost.size == 0:\n",
    "        return []\n",
    "    cost = np.nan_to_num(cost, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "    n,m = cost.shape\n",
    "    N = max(n,m)\n",
    "    pad_val = float(cost.max() * 10.0 if cost.size else 1e9)\n",
    "    pad_cost = np.full((N,N), pad_val, dtype=float)\n",
    "    pad_cost[:n, :m] = cost\n",
    "    pad_cost = np.nan_to_num(pad_cost, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "    row_ind, col_ind = linear_sum_assignment(pad_cost)\n",
    "    pairs = [(r,c) for r,c in zip(row_ind, col_ind) if r < n and c < m]\n",
    "    return pairs\n",
    "\n",
    "def cosine_dists(A, B):\n",
    "    if A.size == 0 or B.size == 0:\n",
    "        return np.zeros((A.shape[0] if A.size else 0, B.shape[0] if B.size else 0))\n",
    "    A_n = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-9)\n",
    "    B_n = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-9)\n",
    "    sims = A_n.dot(B_n.T)\n",
    "    return 1.0 - sims\n",
    "\n",
    "def estimate_scale(pixel_sizes, real_sizes):\n",
    "    pixel = np.array(pixel_sizes, dtype=float)\n",
    "    real = np.array(real_sizes, dtype=float)\n",
    "    mask = (real > 0)\n",
    "    if mask.sum() == 0:\n",
    "        return None\n",
    "    real_m = real[mask]\n",
    "    pixel_m = pixel[mask]\n",
    "    denom = (real_m**2).sum()\n",
    "    if denom == 0:\n",
    "        return None\n",
    "    s = (pixel_m * real_m).sum() / denom\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        return None\n",
    "    return float(s)\n",
    "\n",
    "def embed_images_clips(pil_imgs, batch_size=BATCH_EMBED_SIZE):\n",
    "    if len(pil_imgs) == 0:\n",
    "        return np.zeros((0, 512), dtype=np.float32)\n",
    "    tensors = [clip_preprocess(im) for im in pil_imgs]\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(tensors), batch_size):\n",
    "            batch = torch.stack(tensors[i:i+batch_size]).to(device)\n",
    "            f = clip_model.encode_image(batch)\n",
    "            f = f / (f.norm(dim=-1, keepdim=True) + 1e-9)\n",
    "            embs.append(f.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def embed_texts_clips(texts, batch_size=128):\n",
    "    \"\"\"\n",
    "    Batch text embedding with safety for long titles.\n",
    "    We assume texts are already shortened to be well under CLIP's 77-token limit.\n",
    "    \"\"\"\n",
    "    if len(texts) == 0:\n",
    "        return np.zeros((0, 512), dtype=np.float32)\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            chunk = texts[i:i+batch_size]\n",
    "            try:\n",
    "                toks = clip.tokenize(chunk).to(device)\n",
    "            except RuntimeError as e:\n",
    "                # As a safety net, re-shortening if clip complains\n",
    "                chunk = [shorten_text_for_clip(t, max_words=SHORTEN_WORDS//2) for t in chunk]\n",
    "                toks = clip.tokenize(chunk).to(device)\n",
    "            f = clip_model.encode_text(toks)\n",
    "            f = f / (f.norm(dim=-1, keepdim=True) + 1e-9)\n",
    "            embs.append(f.cpu().numpy())\n",
    "    return np.vstack(embs)\n",
    "\n",
    "def draw_annotations(pil_img, boxes, assignment, asin_to_name, save_path):\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    try:\n",
    "        font = ImageFont.truetype(FONT_FILE, 14)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "    for i, box in enumerate(boxes):\n",
    "        x1,y1,x2,y2 = map(int, box)\n",
    "        asin = assignment.get(i, None)\n",
    "        label = asin_to_name.get(asin, asin) if asin else \"UNKNOWN\"\n",
    "        draw.rectangle([x1,y1,x2,y2], outline=\"red\", width=2)\n",
    "        # background for readability\n",
    "        tw, th = draw.textsize(label, font=font)\n",
    "        draw.rectangle([(x1, y1 - th - 4), (x1 + tw + 4, y1)], fill=(0,0,0))\n",
    "        draw.text((x1+2, y1 - th - 2), label, fill=(255,255,0), font=font)\n",
    "    pil_img.save(save_path)\n",
    "\n",
    "# ---------------- TEXT SHORTENING + CACHING ----------------\n",
    "def shorten_text_for_clip(text, max_words=SHORTEN_WORDS):\n",
    "    \"\"\"Clean and shorten product title for CLIP tokenization.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # remove HTML entities like &reg;\n",
    "    text = re.sub(r\"&[a-zA-Z0-9#]+;\", \" \", text)\n",
    "    # remove non-alphanumeric except common punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-\\.,]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    words = text.split()\n",
    "    short = \" \".join(words[:max_words])\n",
    "    # final cleanup\n",
    "    short = short.strip()\n",
    "    return short if short else text[:200]\n",
    "\n",
    "text_embedding_cache = {}  # maps text -> embedding (numpy array)\n",
    "def get_text_embeddings_cached(texts):\n",
    "    \"\"\"\n",
    "    Given a list of texts (shortened), return embeddings using cache to avoid re-computation.\n",
    "    Returns numpy array of shape (len(texts), dim).\n",
    "    \"\"\"\n",
    "    to_compute = []\n",
    "    idx_map = []\n",
    "    for i, t in enumerate(texts):\n",
    "        key = t\n",
    "        if key in text_embedding_cache:\n",
    "            idx_map.append((i, \"cached\"))\n",
    "        else:\n",
    "            idx_map.append((i, \"compute\"))\n",
    "            to_compute.append(t)\n",
    "    embs = np.zeros((len(texts), 512), dtype=np.float32)\n",
    "    # fill cached\n",
    "    for i, status in idx_map:\n",
    "        if status == \"cached\":\n",
    "            embs[i] = text_embedding_cache[texts[i]]\n",
    "    # compute needed\n",
    "    if to_compute:\n",
    "        computed = embed_texts_clips(to_compute, batch_size=128)\n",
    "        # assign back to cache in order\n",
    "        ci = 0\n",
    "        for i, status in idx_map:\n",
    "            if status == \"compute\":\n",
    "                emb = computed[ci]\n",
    "                embs[i] = emb\n",
    "                text_embedding_cache[texts[i]] = emb\n",
    "                ci += 1\n",
    "    return embs\n",
    "\n",
    "# ---------------- Main: process all images (resume & incremental save) ----------------\n",
    "all_imgs = sorted(list(Path(IMG_DIR).glob(\"*.jpg\")))\n",
    "if len(all_imgs) == 0:\n",
    "    raise FileNotFoundError(f\"No images found in {IMG_DIR}\")\n",
    "\n",
    "# Resume support\n",
    "processed_images = set()\n",
    "if RESULT_CSV.exists():\n",
    "    try:\n",
    "        existing = pd.read_csv(RESULT_CSV)\n",
    "        processed_images = set(existing[\"image_id\"].astype(str).unique().tolist())\n",
    "        print(f\"[info] Resuming. Found existing results with {len(processed_images)} processed images.\")\n",
    "    except Exception:\n",
    "        processed_images = set()\n",
    "\n",
    "results_rows = []\n",
    "counter = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for img_path in tqdm(all_imgs, desc=\"All images\"):\n",
    "    image_id = img_path.stem\n",
    "    if image_id in processed_images:\n",
    "        continue\n",
    "\n",
    "    meta_path = Path(META_DIR) / f\"{image_id}.json\"\n",
    "    if not meta_path.exists():\n",
    "        if VERBOSE:\n",
    "            print(f\"[warn] metadata missing for {image_id}, skipping\")\n",
    "        processed_images.add(image_id)\n",
    "        continue\n",
    "\n",
    "    meta = json.load(open(meta_path, \"r\"))\n",
    "    bin_data = meta.get(\"BIN_FCSKU_DATA\", {})\n",
    "    if not bin_data:\n",
    "        if VERBOSE:\n",
    "            print(f\"[warn] BIN_FCSKU_DATA empty for {image_id}, skipping\")\n",
    "        processed_images.add(image_id)\n",
    "        continue\n",
    "\n",
    "    # Build asin info map & expanded instances\n",
    "    asin_info = {}\n",
    "    expanded_asins = []\n",
    "    asin_to_name = {}\n",
    "    for asin, info in bin_data.items():\n",
    "        asin_info[asin] = info or {}\n",
    "        qty = int(info.get(\"quantity\", 1) or 1)\n",
    "        for _ in range(max(1, qty)):\n",
    "            expanded_asins.append(asin)\n",
    "        asin_to_name[asin] = info.get(\"normalizedName\") or info.get(\"name\") or asin\n",
    "\n",
    "    # YOLO inference\n",
    "    try:\n",
    "        res = yolo(str(img_path), device=device)\n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"[error] YOLO inference failed for {image_id}: {e}\")\n",
    "        processed_images.add(image_id)\n",
    "        continue\n",
    "\n",
    "    r = res[0]\n",
    "    if len(r.boxes) == 0:\n",
    "        for asin, info in asin_info.items():\n",
    "            results_rows.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"asin\": asin,\n",
    "                \"name\": asin_to_name.get(asin),\n",
    "                \"expected_qty\": int(info.get(\"quantity\",1) or 1),\n",
    "                \"detected_qty\": 0,\n",
    "                \"match\": False,\n",
    "                \"scale\": None,\n",
    "                \"notes\": \"no_detections\"\n",
    "            })\n",
    "        if VERBOSE:\n",
    "            print(f\"[info] no detections for {image_id}\")\n",
    "        processed_images.add(image_id)\n",
    "        counter += 1\n",
    "    else:\n",
    "        boxes = r.boxes.xyxy.cpu().numpy().tolist()\n",
    "        pil_img = load_image(img_path)\n",
    "        crops = [crop_box(pil_img, b) for b in boxes]\n",
    "        crop_metrics = [box_metrics(b) for b in boxes]\n",
    "        pixel_size_measure = np.array([cm[\"max_dim\"] for cm in crop_metrics], dtype=float)\n",
    "        pixel_size_measure = np.nan_to_num(pixel_size_measure, nan=1.0, posinf=1e6, neginf=1.0)\n",
    "        pixel_size_measure[pixel_size_measure <= 0] = 1.0\n",
    "\n",
    "        # embed crops (batched)\n",
    "        crop_embs = embed_images_clips(crops)\n",
    "\n",
    "        # prepare and shorten texts for this image, use caching\n",
    "        unique_asins = list(asin_info.keys())\n",
    "        unique_texts_raw = [asin_to_name[a] for a in unique_asins]\n",
    "        unique_texts = [shorten_text_for_clip(t, max_words=SHORTEN_WORDS) for t in unique_texts_raw]\n",
    "        # get embeddings from cache or compute\n",
    "        text_embs_unique = get_text_embeddings_cached(unique_texts)  # u x d\n",
    "        if text_embs_unique.size == 0:\n",
    "            # as fallback use raw ASIN strings\n",
    "            text_embs_unique = get_text_embeddings_cached(unique_asins)\n",
    "\n",
    "        # map expanded instances to embeddings\n",
    "        expanded_text_embs = np.vstack([text_embs_unique[unique_asins.index(a)] for a in expanded_asins]) if expanded_asins else np.zeros((0,512))\n",
    "\n",
    "        # visual cost matrix\n",
    "        visual_cost = cosine_dists(crop_embs, expanded_text_embs)\n",
    "\n",
    "        # compute real sizes\n",
    "        real_sizes = []\n",
    "        for a in expanded_asins:\n",
    "            info = asin_info.get(a, {}) or {}\n",
    "            L = safe_get_info(info, \"length\")\n",
    "            W = safe_get_info(info, \"width\")\n",
    "            H = safe_get_info(info, \"height\")\n",
    "            vals = [v for v in (L,W,H) if v is not None and (isinstance(v,(int,float)) or str(v).replace('.','',1).isdigit())]\n",
    "            try:\n",
    "                vals = [float(v) for v in vals]\n",
    "            except Exception:\n",
    "                vals = []\n",
    "            real_max = float(max(vals)) if vals else 1.0\n",
    "            if not np.isfinite(real_max) or real_max <= 1e-6:\n",
    "                real_max = 1.0\n",
    "            real_sizes.append(real_max)\n",
    "        real_sizes = np.array(real_sizes, dtype=float)\n",
    "\n",
    "        try:\n",
    "            approx_s = (np.median(pixel_size_measure) / np.median(real_sizes)) if real_sizes.size and np.median(real_sizes) > 0 else 1.0\n",
    "        except Exception:\n",
    "            approx_s = 1.0\n",
    "\n",
    "        def size_penalty_matrix(s):\n",
    "            expected = s * real_sizes if real_sizes.size else np.ones((0,))\n",
    "            if expected.size == 0:\n",
    "                return np.zeros((pixel_size_measure.shape[0], 0))\n",
    "            P = np.abs(pixel_size_measure[:, None] - expected[None, :]) / (expected[None, :] + 1e-9)\n",
    "            P = np.nan_to_num(P, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "            return P\n",
    "\n",
    "        # assignment\n",
    "        if visual_cost.size == 0:\n",
    "            assignment = {}\n",
    "            assigned_counts = Counter()\n",
    "            s = None\n",
    "            if VERBOSE:\n",
    "                print(f\"[warn] visual_cost empty for {image_id}\")\n",
    "        else:\n",
    "            pairs = hungarian_from_cost(visual_cost)\n",
    "            matched_pixel = [pixel_size_measure[r] for r,c in pairs] if pairs else []\n",
    "            matched_real = [real_sizes[c] for r,c in pairs] if pairs else []\n",
    "            s = estimate_scale(matched_pixel, matched_real) or approx_s\n",
    "\n",
    "            for it in range(4):\n",
    "                sz = size_penalty_matrix(s)\n",
    "                v_norm = normalize_safe(visual_cost)\n",
    "                sz_norm = normalize_safe(sz)\n",
    "                cost = ALPHA * v_norm + BETA * sz_norm\n",
    "                cost = np.nan_to_num(cost, nan=1e6, posinf=1e6, neginf=1e6)\n",
    "                pairs = hungarian_from_cost(cost)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                matched_pixel = [pixel_size_measure[r] for r,c in pairs]\n",
    "                matched_real = [real_sizes[c] for r,c in pairs]\n",
    "                new_s = estimate_scale(matched_pixel, matched_real)\n",
    "                if new_s is None:\n",
    "                    break\n",
    "                if abs(new_s - s) / (s + 1e-9) < 1e-3:\n",
    "                    s = new_s\n",
    "                    break\n",
    "                s = new_s\n",
    "\n",
    "            assignment = {r: expanded_asins[c] for r,c in pairs} if pairs else {}\n",
    "            assigned_counts = Counter(assignment.values())\n",
    "\n",
    "        # build rows for this image\n",
    "        for asin, info in asin_info.items():\n",
    "            expected_qty = int(info.get(\"quantity\",1) or 1)\n",
    "            detected_qty = assigned_counts.get(asin, 0)\n",
    "            results_rows.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"asin\": asin,\n",
    "                \"name\": asin_to_name.get(asin),\n",
    "                \"expected_qty\": expected_qty,\n",
    "                \"detected_qty\": detected_qty,\n",
    "                \"match\": detected_qty == expected_qty,\n",
    "                \"scale\": float(s) if s is not None else None,\n",
    "                \"notes\": None\n",
    "            })\n",
    "\n",
    "        # save annotated image\n",
    "        out_path = ANNOTATED_DIR / f\"{image_id}_classified.jpg\"\n",
    "        try:\n",
    "            draw_annotations(pil_img, boxes, assignment, asin_to_name, str(out_path))\n",
    "        except Exception as e:\n",
    "            if VERBOSE:\n",
    "                print(f\"[warn] drawing annotations failed for {image_id}: {e}\")\n",
    "\n",
    "        processed_images.add(image_id)\n",
    "        counter += 1\n",
    "\n",
    "    # periodic save\n",
    "    if counter % SAVE_EVERY == 0 and results_rows:\n",
    "        try:\n",
    "            if RESULT_CSV.exists():\n",
    "                df_existing = pd.read_csv(RESULT_CSV)\n",
    "                df_new = pd.DataFrame(results_rows)\n",
    "                df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "                df_combined.to_csv(RESULT_CSV, index=False)\n",
    "            else:\n",
    "                pd.DataFrame(results_rows).to_csv(RESULT_CSV, index=False)\n",
    "            results_rows = []\n",
    "            if VERBOSE:\n",
    "                print(f\"[info] Flushed intermediate results to {RESULT_CSV}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[error] Failed to flush intermediate results: {e}\")\n",
    "\n",
    "# final flush of any remaining rows\n",
    "if results_rows:\n",
    "    if RESULT_CSV.exists():\n",
    "        df_existing = pd.read_csv(RESULT_CSV)\n",
    "        df_new = pd.DataFrame(results_rows)\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(RESULT_CSV, index=False)\n",
    "    else:\n",
    "        pd.DataFrame(results_rows).to_csv(RESULT_CSV, index=False)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n[done] Processed all images in {elapsed:.1f}s. Results -> {RESULT_CSV}; annotated images -> {ANNOTATED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e7b315-4753-439b-8af5-91a5f73b1b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-image accuracy:\n",
      "   image_id  image_accuracy\n",
      "0         3        0.333333\n",
      "1         4        1.000000\n",
      "2        14        1.000000\n",
      "3        15        0.500000\n",
      "4        16        0.500000\n",
      "\n",
      "================ OVERALL DATASET SUMMARY ================\n",
      "Total Images:            49091\n",
      "Total ASIN Entries:      136052\n",
      "Correct Matches:         87764\n",
      "Incorrect Matches:       48288\n",
      "Overall Accuracy:        64.51%\n",
      "=========================================================\n",
      "\n",
      "Per-ASIN accuracy (first 10):\n",
      "         asin  asin_accuracy\n",
      "0  0001388487       0.000000\n",
      "1  000217412X       1.000000\n",
      "2  000255450X       1.000000\n",
      "3  0002624028       1.000000\n",
      "4  0007446993       1.000000\n",
      "5  0008127530       1.000000\n",
      "6  0020435207       1.000000\n",
      "7  002081030X       1.000000\n",
      "8  0021440719       0.333333\n",
      "9  002166790X       1.000000\n",
      "\n",
      "Saved results_with_accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your classified results\n",
    "df = pd.read_csv(\"outputs/classified/results.csv\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 1. IMAGE-LEVEL ACCURACY\n",
    "# -------------------------------------------\n",
    "image_acc = (\n",
    "    df.groupby(\"image_id\")[\"match\"]\n",
    "      .mean()   # average match per image\n",
    "      .reset_index()\n",
    "      .rename(columns={\"match\": \"image_accuracy\"})\n",
    ")\n",
    "\n",
    "print(\"Per-image accuracy:\")\n",
    "print(image_acc.head())\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 2. OVERALL ACCURACY (ACROSS ALL ASIN ENTRIES)\n",
    "# -------------------------------------------\n",
    "total_entries = len(df)\n",
    "total_correct = df[\"match\"].sum()\n",
    "overall_accuracy = total_correct / total_entries if total_entries > 0 else 0\n",
    "\n",
    "print(\"\\n================ OVERALL DATASET SUMMARY ================\")\n",
    "print(f\"Total Images:            {df['image_id'].nunique()}\")\n",
    "print(f\"Total ASIN Entries:      {total_entries}\")\n",
    "print(f\"Correct Matches:         {total_correct}\")\n",
    "print(f\"Incorrect Matches:       {total_entries - total_correct}\")\n",
    "print(f\"Overall Accuracy:        {overall_accuracy*100:.2f}%\")\n",
    "print(\"=========================================================\\n\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 3. ASIN-LEVEL ACCURACY (optional)\n",
    "# -------------------------------------------\n",
    "asin_acc = (\n",
    "    df.groupby(\"asin\")[\"match\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .rename(columns={\"match\": \"asin_accuracy\"})\n",
    ")\n",
    "\n",
    "print(\"Per-ASIN accuracy (first 10):\")\n",
    "print(asin_acc.head(10))\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# 4. MERGE IMAGE-LEVEL ACCURACY BACK\n",
    "# -------------------------------------------\n",
    "df_with_acc = df.merge(image_acc, on=\"image_id\", how=\"left\")\n",
    "\n",
    "# If you want: save a CSV for analysis\n",
    "df_with_acc.to_csv(\"outputs/classified/results_with_accuracy.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved results_with_accuracy.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
